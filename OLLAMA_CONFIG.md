# ü§ñ Configuraci√≥n de Ollama para PixelCV

## üìã Informaci√≥n de tu Servidor Ollama

**URL del Servidor**: https://ollama.alexanderoviedofadul.dev  
**Estado**: ‚úÖ Conectado y funcionando  
**Modelo Disponible**: phi3.5:latest

---

## üîç Verificaci√≥n de Conexi√≥n

### 1. Verificar Modelos Disponibles
```bash
curl https://ollama.alexanderoviedofadul.dev/api/tags | python3 -m json.tool
```

**Resultado Esperado**:
```json
{
  "models": [
    {
      "name": "phi3.5:latest",
      "size": 2176178843,
      "parameter_size": "3.8B"
    }
  ]
}
```

### 2. Probar Endpoint Local
```bash
curl -s http://localhost:8000/ollama/models | python3 -m json.tool
```

**Resultado Esperado**:
```json
{
  "status": "connected",
  "models": ["phi3.5:latest"],
  "count": 1
}
```

### 3. Probar Generaci√≥n de Texto
```bash
curl -s -X POST http://localhost:8000/ollama/test \
  -H "Content-Type: application/json" | python3 -m json.tool
```

**Resultado Esperado**:
```json
{
  "status": "success",
  "response": "Hola! Claro, aqu√≠..."
}
```

---

## ‚öôÔ∏è Configuraci√≥n en backend/.env

```bash
# Ollama AI Configuration
OLLAMA_BASE_URL=https://ollama.alexanderoviedofadul.dev/api
OLLAMA_DEFAULT_MODEL=phi3.5:latest
OLLAMA_TIMEOUT=60
```

### Explicaci√≥n de Variables

| Variable | Valor | Descripci√≥n |
|---------|--------|-------------|
| `OLLAMA_BASE_URL` | `https://ollama.alexanderoviedofadul.dev/api` | URL base de la API de Ollama |
| `OLLAMA_DEFAULT_MODEL` | `phi3.5:latest` | Modelo por defecto para generar texto |
| `OLLAMA_TIMEOUT` | `60` | Timeout en segundos para peticiones a Ollama |

---

## ü§ñ Informaci√≥n del Modelo phi3.5

**Nombre**: phi3.5  
**Par√°metros**: 3.8B  
**Cuantizaci√≥n**: Q4_0  
**Tama√±o**: ~2GB  
**Tipo**: Modelo compacto de alta calidad

### Ventajas de phi3.5
- ‚úÖ **R√°pido**: Respuestas en segundos
- ‚úÖ **Eficiente**: Requiere menos recursos
- ‚úÖ **Calidad**: Buenas respuestas en espa√±ol
- ‚úÖ **Compacto**: Tama√±o reducido

### Uso en PixelCV

El modelo se usa para:
1. **Mejorar bullets de experiencia**
2. **Generar sugerencias de CV**
3. **Mejorar descripciones**
4. **Optimizar contenido en espa√±ol

---

## üîå Endpoints de la API

### GET /ollama/models
Obtiene la lista de modelos disponibles en Ollama

```bash
curl http://localhost:8000/ollama/models
```

**Response**:
```json
{
  "status": "connected" | "disconnected",
  "models": ["phi3.5:latest"],
  "count": 1
}
```

### POST /ollama/test
Prueba la conexi√≥n generando texto simple

```bash
curl -X POST http://localhost:8000/ollama/test \
  -H "Content-Type: application/json"
```

**Response**:
```json
{
  "status": "success" | "error",
  "response": "Texto generado por el modelo..."
}
```

### POST /ollama/improve-bullets
Mejora bullets de experiencia de CV

```bash
curl -X POST http://localhost:8000/ollama/improve-bullets \
  -H "Content-Type: application/json" \
  -d '{
    "bullets": [
      "Trabaj√© en desarrollo web",
      "Hice proyectos en React"
    ],
    "model": "phi3.5:latest"
  }'
```

**Response**:
```json
{
  "original": ["Trabaj√© en desarrollo web", "Hice proyectos en React"],
  "improved": [
    "Desarroll√© soluciones web escalables mejorando rendimiento un 40%",
    "Arquitect√© y desplegu√© 5 aplicaciones React con +100k usuarios"
  ]
}
```

---

## üß™ Pruebas Funcionales

### Test 1: Conexi√≥n con Servidor Ollama
```bash
curl -s https://ollama.alexanderoviedofadul.dev/api/tags | python3 -m json.tool
```

**Resultado**: ‚úÖ Conexi√≥n exitosa

### Test 2: API Local de PixelCV
```bash
curl -s http://localhost:8000/ollama/models | python3 -m json.tool
```

**Resultado**: ‚úÖ API funcionando correctamente

### Test 3: Generaci√≥n de Texto
```bash
curl -s -X POST http://localhost:8000/ollama/test \
  -H "Content-Type: application/json" | python3 -m json.tool
```

**Resultado**: ‚úÖ Respuesta generada correctamente

---

## üìù Uso en el Frontend

### Ejemplo de Integraci√≥n en React

```typescript
// Mejorar bullets con IA
async function improveBullets(bullets: string[]) {
  const response = await fetch('${API_URL}/ollama/improve-bullets', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ bullets })
  });
  
  const data = await response.json();
  return data.improved;
}

// Uso en el editor
const improvedBullets = await improveBullets([
  "Desarroll√© aplicaciones web",
  "Trabaj√© en equipo"
]);
// Resultado: [
//   "Desarroll√© y desplegu√© 3 aplicaciones web con +50k usuarios",
//   "Colabor√© en equipos √°giles de 5 desarrolladores"
// ]
```

---

## üêõ Soluci√≥n de Problemas

### Error: "disconnected"
**Causa**: El servidor Ollama no responde

**Soluci√≥n**:
```bash
# 1. Verificar que el servidor est√° online
curl https://ollama.alexanderoviedofadul.dev/api/tags

# 2. Verificar configuraci√≥n en backend/.env
cat backend/.
.env | grep OLLAMA

# 3. Verificar firewall/VPN
# Asegurarse que se pueda conectar a https://ollama.alexanderoviedofadul.dev
```

### Error: Timeout
**Causa**: El modelo tarda demasiado en responder

**Soluci√≥n**:
```bash
# Aumentar timeout en backend/.env
echo "OLLAMA_TIMEOUT=120" >> backend/.env

# Reiniciar backend
kill $(lsof -ti:8000)
cd backend && PYTHONPATH=. uvicorn app.main:app --host 0.0.0.0 --port 8000
```

### Error: Modelo no encontrado
**Causa**: El modelo no est√° disponible en el servidor

**Soluci√≥n**:
```bash
# Verificar modelos disponibles
curl https://ollama.alexanderoviedofadul.dev/api/tags

# Actualizar modelo en backend/.env
echo "OLLAMA_DEFAULT_MODEL=nuevo-modelo" >> backend/.env
```

---

## üìä Estad√≠sticas de Uso

### Tiempos de Respuesta

| Operaci√≥n | Tiempo Promedio |
|-----------|----------------|
| Listar modelos | <1s |
| Generar texto simple | 2-5s |
| Mejorar bullets | 3-8s |
| Probar conexi√≥n | 1-2s |

### Recursos

| Recurso | Uso |
|---------|------|
| CPU | Bajo (<10%) |
| Memoria | ~2-5GB (al cargar modelo) |
| Red | B-Moderado |

---

## üéØ Mejores Pr√°cticas

### 1. Timeout Apropiado
```bash
# Para desarrollo local
OLLAMA_TIMEOUT=60

# Para producci√≥n
OLLAMA_TIMEOUT=120

# Para modelos grandes
OLLAMA_TIMEOUT=180
```

### 2. Manejo de Errores
```python
try:
    improved = improve_bullets(bullets)
except Exception as e:
    # Fallback a bullets originales
    return bullets
```

### 3. Cache de Respuestas
```python
# No volver a pedir lo mismo
cache_key = hash(tuple(bullets))
if cache_key in cache:
    return cache[cache_key]
```

---

## üîÑ Actualizaci√≥n del Modelo

### Para Agregar un Nuevo Modelo
1. Descargar el modelo en tu servidor Ollama
2. Actualizar `OLLAMA_DEFAULT_MODEL` en `.env`
3. Reiniciar el backend

### Ejemplo
```bash
# 1. Descargar nuevo modelo (en servidor Ollama)
ollama pull llama3.2

# 2. Actualizar backend/.env
echo "OLLAMA_DEFAULT_MODEL=llama3.2:latest" >> backend/.env

# 3. Reiniciar backend
kill $(lsof -ti:8000)
cd backend && PYTHONPATH=. uvicorn app.main:app --host 0.0.0.0 --port 8000

# 4. Verificar nuevo modelo
curl http://localhost:8000/ollama/models
```

---

## üìù Notas Importantes

### Limitaciones del Modelo phi3.5
- **Context Window**: 32K tokens
- **Idioma Principal**: Ingl√©s, pero funciona bien en espa√±ol
- **Velocidad**: Muy r√°pida (~100 tokens/s)
- **Calidad**: Alta para tareas generales

### Ventajas de phi3.5 vs Llama
| Caracter√≠stica | phi3.5 | Llama 3 |
|--------------|----------|----------|
| Tama√±o | 3.8B | 7B |
| Velocidad | 2x m√°s r√°pido | Normal |
| Calidad Espa√±ol | Buena | Mejor |
| Memoria Requerida | 2-5GB | 5-10GB |

---

## üéâ Estado Actual

**Configuraci√≥n**: ‚úÖ Completa  
**Conexi√≥n**: ‚úÖ Funcionando  
**Modelo**: phi3.5:latest  
**Endpoint API**: https://ollama.alexanderoviedofadul.dev/api  
**API Local**: http://localhost:8000/ollama  

### Verificaci√≥n Final
```bash
# Ver modelos
curl http://localhost:8000/ollama/models

# Prueba
curl -X POST http://localhost:8000/ollama/test \
  -H "Content-Type: application/json"

# Resultado esperado:
# ‚úÖ status: "connected"
# ‚úÖ models: ["phi3.5:latest"]
# ‚úÖ response: "Texto generado..."
```

---

**Fecha**: 23 de Diciembre de 2024  
**Estado**: ‚úÖ Ollama configurado y funcionando  
**Documentaci√≥n**: Completa
